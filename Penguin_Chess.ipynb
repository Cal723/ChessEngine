{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mchess\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mboard_to_tensor\u001b[39m(board):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chess'"
     ]
    }
   ],
   "source": [
    "import chess\n",
    "import torch\n",
    "def board_to_tensor(board):\n",
    "    board_tensor = torch.zeros(64, dtype=torch.float32)\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            board_tensor[square] = piece.piece_type if piece.color == chess.WHITE else -piece.piece_type\n",
    "    return board_tensor\n",
    "\n",
    "def get_coordinates(str):\n",
    "    col = ord(str[0])-97\n",
    "    row = int(str[1]) - 1\n",
    "    return row, col\n",
    "\n",
    "def get_row_col(square):\n",
    "    row = square // 8    # Rank (0 is the first row)\n",
    "    col = square % 8     # File (0 is the first column)\n",
    "    return row, col\n",
    "\n",
    "def encode_move(move):\n",
    "    from_square = move.from_square\n",
    "    to_square = move.to_square\n",
    "    \n",
    "    #start_row, start_col = get_coordinates(from_square)\n",
    "    #end_row, end_col = get_coordinates(to_square)\n",
    "    start_row, start_col = get_row_col(from_square)\n",
    "    end_row, end_col = get_row_col(to_square)\n",
    "\n",
    "    return (8*64*(start_row) + 64*(start_col) + 8*(end_row)+end_col) + 1\n",
    "\n",
    "    \n",
    "board = chess.Board()\n",
    "print(board)\n",
    "legal_moves = list(board.legal_moves)\n",
    "for move in legal_moves:\n",
    "    from_square = move.from_square\n",
    "    to_square = move.to_square\n",
    "    piece = board.piece_at(from_square)\n",
    "    #print(f\"Move {board.san(move)}:\")\n",
    "    #print(f\"  Piece: {piece}\")\n",
    "    #print(f\"  From: {chess.square_name(from_square)}\")\n",
    "    #print(get_coordinates(chess.square_name(from_square)))\n",
    "    #print(f\"  To: {chess.square_name(to_square)}\")\n",
    "    \n",
    "print(board_to_tensor(board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 4096)\n",
    "\n",
    "    def forward(self, x, turn):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "q_network = DQN()\n",
    "target_network = DQN()\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(q_network.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#replay_buffer = ReplayBuffer(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Experience replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, turn, state, action, reward, next_state, done):\n",
    "        self.buffer.append((turn, state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "\n",
    "\n",
    "def select_action(board, state, q_network, turn, epsilon):\n",
    "    legal_moves = list(board.legal_moves)\n",
    "    if random.random() < epsilon:\n",
    "        # Exploration\n",
    "        move = random.choice(legal_moves)\n",
    "        print(\"random legal move\")\n",
    "        print(move)\n",
    "        return encode_move(move)\n",
    "    else:\n",
    "        # Exploitation\n",
    "        print(\"using predicted move\")\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        q_values = q_network(state_tensor)\n",
    "        \n",
    "        # Create a mask to zero out illegal moves\n",
    "        #mask = torch.zeros_like(q_values)\n",
    "        #legal_moves_indices = [env.move_to_index(move) for move in legal_moves]\n",
    "        #mask[legal_moves_indices] = 1\n",
    "        \n",
    "        # Apply the mask to the Q-values\n",
    "        #masked_q_values = q_values * mask\n",
    "        \n",
    "        # Select the move with the highest masked Q-value\n",
    "        best_move_index = torch.argmax(q_values).item() + 1\n",
    "        \n",
    "        \n",
    "        return best_move_index\n",
    "\n",
    "\n",
    "def train(replay_buffer, batch_size, gamma):\n",
    "    if replay_buffer.size() < batch_size:\n",
    "        return\n",
    "    batch = replay_buffer.sample(batch_size)\n",
    "    turns, states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    \n",
    "    states = torch.stack(states)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "    next_states = torch.stack(next_states)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "    q_values = q_network(states).gather(1,actions).squeeze(1)\n",
    "    next_q_values = target_network(next_states).max(1)[0]\n",
    "    expected_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "    \n",
    "    \n",
    "    loss = criterion(q_values, expected_q_values.detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "turn = 1\n",
    "num_episodes = 1000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    board = chess.Board()\n",
    "    state = board_to_tensor(board)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        reward = 0\n",
    "        epsilon = epsilon_end + (epsilon_start - epsilon_end) * torch.exp(torch.tensor(-1. * episode / epsilon_decay)).item()\n",
    "\n",
    "        \n",
    "        best_move_index = select_action(board, state, q_network, turn, epsilon)\n",
    "        \n",
    "        start_square = (best_move_index//64)\n",
    "        end_square = (best_move_index % 64) -1\n",
    "        move = chess.Move(start_square, end_square)\n",
    "        #print(move)\n",
    "        \n",
    "        \n",
    "        board.push(move)\n",
    "        print(board)\n",
    "        action = best_move_index\n",
    "        \n",
    "        next_state = board_to_tensor(board)\n",
    "        \n",
    "        if board.is_game_over():\n",
    "            if board.result() == \"1-0\":\n",
    "                reward = 100 * turn\n",
    "            elif board.result() == \"0-1\":\n",
    "                reward = -100 * turn\n",
    "            done = True\n",
    "        \n",
    "        replay_buffer.add(turn, state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        train(replay_buffer, batch_size, gamma)\n",
    "        turn = turn * -1\n",
    "    if episode % 10 == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
